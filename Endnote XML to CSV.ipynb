{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import google.generativeai as genai\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_abstract_for_algorithm(abstract):\n",
    "    genai.configure(api_key='your_api_key')\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze this abstract and determine if it reports a delirium prediction or detection algorithm.\n",
    "    Answer only 'yes' if it describes developing or validating a prediction model, machine learning algorithm, \n",
    "    or detection system for delirium. Answer 'no' otherwise.\n",
    "    \n",
    "    Abstract: {abstract}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip().lower() == 'yes'\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing abstract: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_endnote_xml_to_df(xml_path):\n",
    "    \"\"\"\n",
    "    Parse EndNote XML export file and extract publication details into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the data\n",
    "    data = {\n",
    "        'Authors': [],\n",
    "        'Year': [],\n",
    "        'Title': [],\n",
    "        'DOI': [],\n",
    "        'Journal': [],\n",
    "        'Abstract': []\n",
    "    }\n",
    "    \n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Find all record elements\n",
    "    for record in root.findall('.//record'):\n",
    "        try:\n",
    "            # Extract authors\n",
    "            authors = []\n",
    "            authors_elem = record.find('.//authors')\n",
    "            if authors_elem is not None:\n",
    "                for author in authors_elem.findall('.//author'):\n",
    "                    if author.find('.//style') is not None:\n",
    "                        authors.append(author.find('.//style').text)\n",
    "            \n",
    "            # Extract year\n",
    "            year_elem = record.find('.//dates/year/style')\n",
    "            year = year_elem.text if year_elem is not None else ''\n",
    "            \n",
    "            # Extract title\n",
    "            title_elem = record.find('.//titles/title/style')\n",
    "            title = title_elem.text if title_elem is not None else ''\n",
    "            \n",
    "            # Extract DOI\n",
    "            doi_elem = record.find('.//electronic-resource-num/style')\n",
    "            doi = doi_elem.text if doi_elem is not None else ''\n",
    "            if doi.startswith('https://dx.doi.org/'):\n",
    "                doi = doi[len('https://dx.doi.org/'):]\n",
    "            \n",
    "            # Extract journal name\n",
    "            journal_elem = record.find('.//secondary-title/style')\n",
    "            if journal_elem is None:\n",
    "                journal_elem = record.find('.//periodical/full-title/style')\n",
    "            journal = journal_elem.text if journal_elem is not None else ''\n",
    "            \n",
    "            # Extract abstract\n",
    "            abstract_elem = record.find('.//abstract/style')\n",
    "            abstract = abstract_elem.text if abstract_elem is not None else ''\n",
    "            \n",
    "            # Append to data dictionary\n",
    "            data['Authors'].append('; '.join(authors))\n",
    "            data['Year'].append(year)\n",
    "            data['Title'].append(title)\n",
    "            data['DOI'].append(doi)\n",
    "            data['Journal'].append(journal)\n",
    "            data['Abstract'].append(abstract)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing a record: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add algorithm analysis column\n",
    "    print(\"Analyzing abstracts for algorithms...\")\n",
    "    df['algorithm'] = df['Abstract'].apply(analyze_abstract_for_algorithm)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "try:\n",
    "    xml_path = r\"your_endnote_export.xml\"\n",
    "    df = parse_endnote_xml_to_df(xml_path)\n",
    "    \n",
    "    # Basic data inspection\n",
    "    print(f\"\\nNumber of articles found: {len(df)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Optional: Save to CSV\n",
    "    # df.to_csv('publication_data_with_algorithm.csv', index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe where algorithm is True\n",
    "algorithm_df = df[df['algorithm'] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DOI link column\n",
    "algorithm_df['DOI_Link'] = algorithm_df['DOI'].apply(lambda x: f'https://doi.org/{x}' if x else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and reorder columns, excluding abstract\n",
    "final_df = algorithm_df[['Authors', 'Year', 'Title', 'Journal', 'DOI', 'DOI_Link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the filtered dataframe\n",
    "print(f\"Number of papers with algorithms: {len(final_df)}\")\n",
    "print(\"\\nFirst few algorithm papers:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "csv_path = 'delirium_algorithm_papers.csv'\n",
    "final_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nExported to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display the count by year\n",
    "year_count = final_df['Year'].value_counts().sort_index()\n",
    "print(\"\\nPapers by year:\")\n",
    "print(year_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
