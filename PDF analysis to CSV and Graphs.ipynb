{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf pandas your-google-generativeai matplotlib seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_required_packages():\n",
    "    \"\"\"Install required packages\"\"\"\n",
    "    required_packages = {\n",
    "        'pypdf': 'pypdf',\n",
    "        'pandas': 'pandas',\n",
    "        'your-google-generativeai': 'your-google-generativeai',\n",
    "        'python-dotenv': 'python-dotenv'\n",
    "    }\n",
    "    \n",
    "    for package_name, pip_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package_name.replace('-', ''))\n",
    "            print(f\"{package_name} is already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package_name}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "            print(f\"Successfully installed {package_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "install_required_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now proceed with the rest of the imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = \"your-base-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable for LLM\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dependencies():\n",
    "    \"\"\"Check if required packages are installed\"\"\"\n",
    "    required_packages = {\n",
    "        'pypdf': 'pypdf',\n",
    "        'pandas': 'pandas',\n",
    "        'google.generativeai': 'your-google-generativeai'  # Changed from 'your-google-generativeai' to 'google.generativeai'\n",
    "    }\n",
    "    \n",
    "    missing_packages = []\n",
    "    for package, pip_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package.split('.')[0])  # Split on '.' and import first part for nested packages\n",
    "        except ImportError:\n",
    "            missing_packages.append(pip_name)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(\"Missing required packages. Please install them using:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_genai(api_key: str):\n",
    "    \"\"\"Set up the Google Generative AI model\"\"\"\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "        return genai.GenerativeModel('gemini-1.5-flash', \n",
    "                                   generation_config=genai.GenerationConfig(temperature=0))\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up GenAI: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_api_key(key: str):\n",
    "    \"\"\"Set up the API key and initialize LLM\"\"\"\n",
    "    global llm\n",
    "    llm = setup_genai(key)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: str) -> Optional[str]:\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    try:\n",
    "        from pypdf import PdfReader\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(text: str, llm) -> str:\n",
    "    \"\"\"Extract the title of the study\"\"\"\n",
    "    prompt = f\"\"\"Analyze this research paper text and extract ONLY the main title.\n",
    "    Return just the title, nothing else.\n",
    "    If no title can be found, return 'Not found'.\n",
    "\n",
    "    Text to analyze:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception:\n",
    "        return 'Not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_author(text: str, llm) -> str:\n",
    "    \"\"\"Extract the first author's last name\"\"\"\n",
    "    prompt = f\"\"\"Analyze this research paper text and extract ONLY the last name of the first author.\n",
    "    Return just the last name, nothing else.\n",
    "    If no author can be found, return 'Not found'.\n",
    "\n",
    "    Text to analyze:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception:\n",
    "        return 'Not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_count(text: str, llm) -> str:\n",
    "    \"\"\"Extract number of features used in the model\"\"\"\n",
    "    prompt = f\"\"\"Analyze this research paper and find the number of features or variables used in the final predictive model.\n",
    "    Return ONLY the number.\n",
    "    Look for phrases like 'features', 'variables', 'predictors', 'input parameters'.\n",
    "    If multiple numbers are mentioned, return the final number used in the model.\n",
    "    If no specific number is found, return 'Not reported'.\n",
    "\n",
    "    Text to analyze:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        number = re.search(r'\\d+', response.text)\n",
    "        return number.group() if number else 'Not reported'\n",
    "    except Exception:\n",
    "        return 'Not reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_outcome_variable(text: str, llm) -> str:\n",
    "    \"\"\"Extract the outcome variable of the model\"\"\"\n",
    "    prompt = f\"\"\"Analyze this research paper and identify the main outcome variable that the model is trying to predict.\n",
    "    Return ONLY the outcome in 1-2 words (e.g., 'incident delirium', 'mortality', 'ICU readmission').\n",
    "    Look for phrases like 'outcome variable', 'target variable', 'predicted variable', 'dependent variable'.\n",
    "    If not clearly specified, return 'Not specified'.\n",
    "\n",
    "    Text to analyze:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text.strip()[:50]\n",
    "    except Exception:\n",
    "        return 'Not specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_publication_year_from_pdf(text: str, llm) -> str:\n",
    "    \"\"\"Extract publication year with enhanced pattern matching\"\"\"\n",
    "    # First try exact patterns in the text itself without LLM\n",
    "    text_sample = text[:10000].lower()\n",
    "    \n",
    "    # Common patterns for publication years\n",
    "    patterns = [\n",
    "        r'copyright.*?(20\\d{2})',\n",
    "        r'published.*?(20\\d{2})',\n",
    "        r'publication\\s+date.*?(20\\d{2})',\n",
    "        r'accepted.*?(20\\d{2})',\n",
    "        r'received.*?(20\\d{2})',\n",
    "        r'\\b(20\\d{2})\\s*elsevier',\n",
    "        r'\\b(20\\d{2})\\s*springer',\n",
    "        r'\\b(20\\d{2})\\s*wiley',\n",
    "        r'©\\s*(20\\d{2})',\n",
    "        r'doi:.*?(20\\d{2})',\n",
    "        r'volume.*?(20\\d{2})',\n",
    "        r'\\b(20\\d{2})\\s*by\\s+the\\s+authors',\n",
    "        r'published online.*?(20\\d{2})',\n",
    "        r'\\b(20\\d{2})\\s*;',\n",
    "        r'\\b(20\\d{2})\\s*\\.',\n",
    "        r'first published.*?(20\\d{2})',\n",
    "    ]\n",
    "    \n",
    "    years_found = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text_sample)\n",
    "        years_found.extend([int(y) for y in matches if 2000 <= int(y) <= 2024])\n",
    "    \n",
    "    # If found years through patterns, return the most recent one\n",
    "    if years_found:\n",
    "        return str(max(years_found))\n",
    "    \n",
    "    # If no years found through patterns, try LLM with more specific prompt\n",
    "    prompt = f\"\"\"Analyze this academic paper text and find the publication year.\n",
    "    Look for ALL of these patterns:\n",
    "    1. Copyright statements (e.g., \"© 2023\")\n",
    "    2. Publication dates (e.g., \"Published: 2023\", \"First published 2023\")\n",
    "    3. Received/Accepted/Published dates\n",
    "    4. Journal citations with years\n",
    "    5. DOI dates\n",
    "    6. Volume/Issue dates\n",
    "    7. Footer/Header dates\n",
    "    8. Conference proceedings dates\n",
    "    9. Online publication dates\n",
    "    10. Publisher copyright statements\n",
    "    \n",
    "    Return ONLY a 4-digit year between 2000-2024.\n",
    "    If multiple years found, return the latest publication year.\n",
    "    If no clear publication year found, return 'Not found'.\n",
    "\n",
    "    Text to analyze:\n",
    "    {text[:10000]}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        year_match = re.search(r'\\b(20\\d{2})\\b', response.text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(1))\n",
    "            if 2000 <= year <= 2024:\n",
    "                return str(year)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM year extraction: {e}\")\n",
    "    \n",
    "    # If still not found, try one more pattern set\n",
    "    final_patterns = [\n",
    "        r'20\\d{2}\\s*nov',\n",
    "        r'20\\d{2}\\s*dec',\n",
    "        r'20\\d{2}\\s*jan',\n",
    "        r'20\\d{2}\\s*feb',\n",
    "        r'20\\d{2}\\s*mar',\n",
    "        r'20\\d{2}\\s*apr',\n",
    "        r'20\\d{2}\\s*may',\n",
    "        r'20\\d{2}\\s*jun',\n",
    "        r'20\\d{2}\\s*jul',\n",
    "        r'20\\d{2}\\s*aug',\n",
    "        r'20\\d{2}\\s*sep',\n",
    "        r'20\\d{2}\\s*oct'\n",
    "    ]\n",
    "    \n",
    "    years_found = []\n",
    "    for pattern in final_patterns:\n",
    "        matches = re.findall(pattern, text_sample)\n",
    "        years_found.extend([int(y[:4]) for y in matches if 2000 <= int(y[:4]) <= 2024])\n",
    "    \n",
    "    if years_found:\n",
    "        return str(max(years_found))\n",
    "    \n",
    "    return 'Not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yes_no(text):\n",
    "    \"\"\"Extract yes/no answer from text with comprehensive pattern matching and scoring\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    score = 0\n",
    "    \n",
    "    # Strong negative patterns [-2 points each]\n",
    "    strong_negative = [\n",
    "        'no evidence', 'not mentioned', 'not addressed', 'not discussed',\n",
    "        'not described', 'not implemented', 'not provided', 'not included',\n",
    "        'lacks', 'missing', 'absent', 'none', 'not found', 'did not',\n",
    "        'does not', 'has not', 'have not', 'was not', 'were not',\n",
    "        'no information', 'no data', 'no details', 'no assessment',\n",
    "        'no analysis', 'no evaluation', 'no consideration', 'no documentation',\n",
    "        'failed to', 'neglected to', 'omitted', 'excluded'\n",
    "    ]\n",
    "    \n",
    "    # Weak negative patterns [-1 point each]\n",
    "    weak_negative = [\n",
    "        'limited', 'insufficient', 'inadequate', 'unclear how',\n",
    "        'not clear', 'not specified', 'vague', 'ambiguous',\n",
    "        'minimal', 'poor', 'lacking', 'without'\n",
    "    ]\n",
    "    \n",
    "    # Strong positive patterns [+2 points each]\n",
    "    strong_positive = [\n",
    "        'clearly described', 'well documented', 'explicitly stated',\n",
    "        'thoroughly', 'comprehensively', 'extensively', 'detailed',\n",
    "        'implemented', 'established', 'confirmed', 'validated',\n",
    "        'demonstrated', 'proved', 'showed', 'revealed', 'identified',\n",
    "        'verified', 'conducted', 'performed', 'executed', 'completed',\n",
    "        'achieved', 'succeeded in', 'accomplished'\n",
    "    ]\n",
    "    \n",
    "    # Weak positive patterns [+1 point each]\n",
    "    weak_positive = [\n",
    "        'discussed', 'mentioned', 'noted', 'indicated', 'suggested',\n",
    "        'proposed', 'considered', 'addressed', 'included', 'incorporated',\n",
    "        'integrated', 'utilized', 'employed', 'applied', 'attempted',\n",
    "        'presented', 'reported', 'described'\n",
    "    ]\n",
    "    \n",
    "    # Context-specific positive indicators [+2 points each]\n",
    "    context_positive = {\n",
    "        'validation': ['externally validated', 'cross-validated', 'test set', 'validation cohort'],\n",
    "        'methodology': ['methods include', 'methodology described', 'statistical analysis', 'approach involved'],\n",
    "        'results': ['results show', 'findings indicate', 'analysis revealed', 'study found'],\n",
    "        'implementation': ['implemented in', 'deployed', 'put into practice', 'integrated into workflow'],\n",
    "        'evaluation': ['evaluated using', 'assessed through', 'measured by', 'tested with']\n",
    "    }\n",
    "    \n",
    "    # Check patterns and calculate score\n",
    "    for pattern in strong_negative:\n",
    "        if pattern in text_lower:\n",
    "            score -= 2\n",
    "    \n",
    "    for pattern in weak_negative:\n",
    "        if pattern in text_lower:\n",
    "            score -= 1\n",
    "    \n",
    "    for pattern in strong_positive:\n",
    "        if pattern in text_lower:\n",
    "            score += 2\n",
    "    \n",
    "    for pattern in weak_positive:\n",
    "        if pattern in text_lower:\n",
    "            score += 1\n",
    "    \n",
    "    for patterns in context_positive.values():\n",
    "        for pattern in patterns:\n",
    "            if pattern in text_lower:\n",
    "                score += 2\n",
    "    \n",
    "    # Additional checks\n",
    "    active_voice_patterns = [\n",
    "        'we implemented', 'authors developed', 'study implemented',\n",
    "        'researchers conducted', 'team performed', 'analysis showed'\n",
    "    ]\n",
    "    for pattern in active_voice_patterns:\n",
    "        if pattern in text_lower:\n",
    "            score += 1\n",
    "    \n",
    "    if ' therefore ' in text_lower or ' thus ' in text_lower or ' hence ' in text_lower:\n",
    "        score += 1\n",
    "    \n",
    "    if any(char.isdigit() for char in text):\n",
    "        score += 1\n",
    "    \n",
    "    # Decision thresholds\n",
    "    if not text.strip() or len(text.split()) < 5:\n",
    "        return \"No\"\n",
    "    \n",
    "    if score <= -2:\n",
    "        return \"No\"\n",
    "    elif score >= 2:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_subquestion(text: str, question: str, llm) -> str:\n",
    "    \"\"\"Analyze a single subquestion using the detailed yes/no extraction\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following text, answer this specific question with a detailed explanation:\n",
    "    Question: {question}\n",
    "    \n",
    "    If the information is not explicitly mentioned or is unclear, explain what is missing.\n",
    "    \n",
    "    Text to analyze:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return extract_yes_no(response.text)\n",
    "    except Exception:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_paper(pdf_path: str, llm) -> Dict:\n",
    "    \"\"\"Analyze a single paper and return all results\"\"\"\n",
    "    # Extract text from PDF\n",
    "    text = process_pdf(pdf_path)\n",
    "    if text is None:\n",
    "        return None\n",
    "  \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Extract basic information\n",
    "    results['title'] = extract_title(text, llm)\n",
    "    results['first_author'] = extract_first_author(text, llm)\n",
    "    results['publication_year'] = extract_publication_year_from_pdf(text, filename, llm)\n",
    "    results['feature_count'] = extract_feature_count(text, llm)\n",
    "    results['outcome_variable'] = extract_outcome_variable(text, llm)\n",
    "    \n",
    "    # Analyze main questions\n",
    "    for main_question in DETAILED_QUESTIONS.keys():\n",
    "        column_name = f\"Main__{main_question.split('?')[0]}\"\n",
    "        results[column_name] = analyze_subquestion(text, main_question, llm)\n",
    "    \n",
    "    # Analyze subquestions\n",
    "    for main_question, subquestions in DETAILED_QUESTIONS.items():\n",
    "        for subq in subquestions:\n",
    "            column_name = f\"Sub__{main_question.split('?')[0]}__{subq.split('?')[0]}\"\n",
    "            results[column_name] = analyze_subquestion(text, subq, llm)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_list: List[Dict], output_path: str):\n",
    "    \"\"\"Save results to CSV file\"\"\"\n",
    "    df = pd.DataFrame(results_list)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_statistics(df: pd.DataFrame):\n",
    "    \"\"\"Print summary statistics for each question\"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in ['title', 'first_author', 'feature_count', 'outcome_variable']:\n",
    "            yes_count = (df[col] == 'Yes').sum()\n",
    "            total = len(df)\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"Yes: {yes_count} ({(yes_count/total*100):.1f}%)\")\n",
    "            print(f\"No: {total-yes_count} ({((total-yes_count)/total*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions dictionary with main questions and their subquestions\n",
    "DETAILED_QUESTIONS = {\n",
    "    \"What is the specific purpose and context of the AI model in delirium prediction?\": [\n",
    "        \"Why was the model built?\",\n",
    "        \"Was there a specific gap in the literature that it aimed to address?\",\n",
    "        \"Does the model focus on prevalent vs. incident delirium?\",\n",
    "        \"What is the prediction window?\",\n",
    "        \"What type of ICU population is targeted?\"\n",
    "    ],\n",
    "    \"How was the model developed, and what data were used for training?\": [\n",
    "        \"What data source was used for training the model?\",\n",
    "        \"What is the distribution of the data?\",\n",
    "        \"Is the data representative of the target population?\",\n",
    "        \"What were the steps or methods used to define and select the features?\",\n",
    "        \"How were missing data and outliers assessed and managed?\",\n",
    "        \"What was the gold standard for delirium?\",\n",
    "        \"What type of ML models were tested?\",\n",
    "        \"How was hyperparameter tuning performed?\",\n",
    "        \"Was cross-validation done? What type and how many fold?\"\n",
    "    ],\n",
    "    \"Has the model been externally validated, and how does it perform in different clinical settings?\": [\n",
    "        \"Is the model externally validated?\",\n",
    "        \"What are the performance metrics?\",\n",
    "        \"What are the subgroup analysis performance metrics?\",\n",
    "        \"Are fairness metrics reported?\",\n",
    "        \"Was clinical utility tested?\"\n",
    "    ],\n",
    "    \"How interpretable are the model's outputs, and can clinicians understand the reasoning behind predictions?\": [\n",
    "        \"Have they looked at results that are wrong and tried to understand the reasoning behind the mistakes?\",\n",
    "        \"Report SHAP values or other feature ranking\"\n",
    "    ],\n",
    "    \"Are there any ethical, legal, or social concerns related to the use of the model?\": [\n",
    "        \"Has the paper discussed any of these aspects?\",\n",
    "        \"If so, which aspect and how the paper tried to address it?\"\n",
    "    ],\n",
    "    \"What training and support will be provided to clinicians to effectively use and interpret the model's predictions?\": [\n",
    "        \"Has the paper discussed training and support?\",\n",
    "        \"If so, what steps will be taken to allow the clinician to use the model and understand the model's interpretation output?\"\n",
    "    ],\n",
    "    \"How does the model integrate into existing clinical workflows and complement current practices?\": [\n",
    "        \"Do the authors describe the role of the model in clinical practice?\",\n",
    "        \"Are there steps on how the model should be used in a clinical setting? If yes, what are the steps?\"\n",
    "    ],\n",
    "    \"Has the use of the model been shown to improve patient care and outcomes in prospective studies?\": [\n",
    "        \"Has the paper reported any prospective studies on patient outcomes?\",\n",
    "        \"If so, what were the key findings regarding patient care improvement?\"\n",
    "    ],\n",
    "    \"What are the potential risks or harms associated with implementing the model in clinical practice?\": [\n",
    "        \"Has a risk assessment been done?\",\n",
    "        \"If so, what are the identified potential risks and harm?\"\n",
    "    ],\n",
    "    \"How will the model be maintained and updated over time to ensure continued accuracy and relevance?\": [\n",
    "        \"Has the life cycle of the model been discussed?\",\n",
    "        \"Are there specific plans for model updates and maintenance?\"\n",
    "    ],\n",
    "    \"What measures are in place to monitor the model's performance?\": [\n",
    "        \"Are there monitoring systems after implementation suggested in the paper? If so, what are they?\",\n",
    "        \"How frequently is the model's performance evaluated?\"\n",
    "    ],\n",
    "    \"How does the model compare to existing clinical methods for delirium prediction?\": [\n",
    "        \"Was model performance in clinical practice compared with routine clinical practice?\",\n",
    "        \"Was the Cost:Benefit ratio mentioned? If, so what was it, clinical as well as technical?\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Check dependencies\n",
    "        check_dependencies()\n",
    "        \n",
    "        # Set up paths using global config\n",
    "        pdf_directory = os.path.join(BASE_DIR, \"algorithm pdfs\")\n",
    "        output_csv = os.path.join(BASE_DIR, \"detailed_analysis_results.csv\")\n",
    "        \n",
    "        # Verify directories exist\n",
    "        if not os.path.exists(pdf_directory):\n",
    "            raise FileNotFoundError(f\"PDF directory not found: {pdf_directory}\")\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            raise FileNotFoundError(\"No PDF files found in directory\")\n",
    "        \n",
    "        print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "        \n",
    "        # Process all PDFs\n",
    "        results = []\n",
    "        for i, pdf_file in enumerate(pdf_files, 1):\n",
    "            print(f\"Processing file {i}/{len(pdf_files)}: {pdf_file}\")\n",
    "            pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "            paper_results = analyze_single_paper(pdf_path, llm)\n",
    "            if paper_results:\n",
    "                paper_results['File'] = pdf_file\n",
    "                results.append(paper_results)\n",
    "            else:\n",
    "                print(f\"Warning: Could not process {pdf_file}\")\n",
    "        \n",
    "        if not results:\n",
    "            raise ValueError(\"No results were generated from any PDFs\")\n",
    "        \n",
    "        # Save and analyze results\n",
    "        df = save_results(results, output_csv)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nAnalysis Summary:\")\n",
    "        print(f\"Total papers processed: {len(results)} out of {len(pdf_files)}\")\n",
    "        print_summary_statistics(df)\n",
    "        \n",
    "        # Print column names for verification\n",
    "        print(\"\\nColumns in output:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"- {col}\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this instead of the if __name__ == \"__main__\" block\n",
    "try:\n",
    "    # Use the global BASE_DIR\n",
    "    env_path = os.path.join(BASE_DIR, 'google_api_key.env')\n",
    "    \n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(env_path)\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if api_key:\n",
    "            print(\"Using API key from google_api_key.env\")\n",
    "            set_api_key(api_key)\n",
    "        else:\n",
    "            print(\"GOOGLE_API_KEY not found in google_api_key.env file\")\n",
    "            print(\"Please create google_api_key.env file with your API key\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Environment file not found at: {env_path}\")\n",
    "        print(\"Please create google_api_key.env file with your API key\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Run the main analysis\n",
    "    df = main()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in setup: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "df.to_csv(os.path.join(BASE_DIR, \"delirium_analysis_results.csv\"), index=False)\n",
    "\n",
    "# If you want to verify\n",
    "print(f\"DataFrame saved to: {os.path.join(BASE_DIR, 'delirium_analysis_results.csv')}\")\n",
    "\n",
    "# Optional: Display first few rows to verify\n",
    "print(\"\\nFirst few rows of the saved data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Display shape of the DataFrame\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving plots\n",
    "save_dir = os.path.join(BASE_DIR, 'analysis_plots')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_main_questions(df, save_path):\n",
    "    # Plot main questions\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    main_questions = [col for col in df.columns if col.startswith('Main__')]\n",
    "    yes_percentages = df[main_questions].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "    labels = [q.replace('Main__', '') for q in main_questions]\n",
    "    \n",
    "    # Reverse order\n",
    "    yes_percentages = yes_percentages[::-1]\n",
    "    labels = labels[::-1]\n",
    "    \n",
    "    bars = plt.barh(range(len(labels)), yes_percentages)\n",
    "    for i, v in enumerate(yes_percentages):\n",
    "        plt.text(v + 1, i, f'{v:.1f}%', va='center')\n",
    "    \n",
    "    plt.yticks(range(len(labels)), labels, wrap=True)\n",
    "    plt.xlabel('Percentage of \"Yes\" Responses')\n",
    "    plt.title('Percentage of \"Yes\" Responses for Main Questions')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(save_path, 'main_questions.png'), \n",
    "                dpi=300, \n",
    "                bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_subquestions(df, main_q, save_path):\n",
    "    sub_cols = [col for col in df.columns if col.startswith(f'Sub__{main_q}')]\n",
    "    yes_percentages = df[sub_cols].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "    labels = [col.split('__')[-1] for col in sub_cols]\n",
    "    \n",
    "    # Reverse order\n",
    "    yes_percentages = yes_percentages[::-1]\n",
    "    labels = labels[::-1]\n",
    "    \n",
    "    # Set consistent bar height\n",
    "    bar_height = 0.8\n",
    "    spacing = 1.5\n",
    "    total_height = len(labels) * spacing\n",
    "    \n",
    "    plt.figure(figsize=(12, total_height))\n",
    "    y_positions = np.arange(len(labels)) * spacing\n",
    "    bars = plt.barh(y_positions, yes_percentages, height=bar_height)\n",
    "    \n",
    "    for i, v in enumerate(yes_percentages):\n",
    "        plt.text(v + 1, y_positions[i], f'{v:.1f}%', va='center')\n",
    "    \n",
    "    plt.yticks(y_positions, labels, wrap=True)\n",
    "    plt.xlabel('Percentage of \"Yes\" Responses')\n",
    "    plt.title(f'Subquestions Analysis: {main_q}')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(save_path, f'subquestions_{main_q}.png'), \n",
    "                dpi=300, \n",
    "                bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot main questions\n",
    "print(\"Main Questions Analysis:\")\n",
    "plot_main_questions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving main questions plot...\n"
     ]
    }
   ],
   "source": [
    "# Save main questions plot\n",
    "print(\"Saving main questions plot...\")\n",
    "plot_and_save_main_questions(df, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subquestions for each main category\n",
    "print(\"\\nSubquestions Analysis by Category:\")\n",
    "main_categories = []\n",
    "for col in df.columns:\n",
    "    if col.startswith('Sub__'):\n",
    "        category = col.split('__')[1].split('_')[0]\n",
    "        if category not in main_categories:\n",
    "            main_categories.append(category)\n",
    "\n",
    "for category in main_categories:\n",
    "    plot_subquestions_ordered(df, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save subquestions plots\n",
    "print(\"\\nSaving subquestions plots...\")\n",
    "main_categories = []\n",
    "for col in df.columns:\n",
    "    if col.startswith('Sub__'):\n",
    "        category = col.split('__')[1].split('_')[0]\n",
    "        if category not in main_categories:\n",
    "            main_categories.append(category)\n",
    "\n",
    "for category in main_categories:\n",
    "    print(f\"Processing {category}...\")\n",
    "    plot_and_save_subquestions(df, category, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_summary_heatmap(df, save_path):\n",
    "    main_categories = []\n",
    "    summary_data = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col.startswith('Sub__'):\n",
    "            category = col.split('__')[1].split('_')[0]\n",
    "            if category not in main_categories:\n",
    "                main_categories.append(category)\n",
    "    \n",
    "    for category in main_categories:\n",
    "        sub_cols = [col for col in df.columns if col.startswith(f'Sub__{category}')]\n",
    "        completeness = (df[sub_cols] == 'Yes').mean(axis=1) * 100\n",
    "        summary_data.append(completeness)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data).T\n",
    "    summary_df.columns = main_categories\n",
    "    summary_df.index = df['title']\n",
    "    \n",
    "    plt.figure(figsize=(15, 35))\n",
    "    colors = ['#ffffff', '#e6f3ff', '#bde0ff', '#94cdff', '#6abaff', '#41a7ff', '#1794ff', '#0077e6', '#005bb3', '#003d80']\n",
    "    custom_cmap = sns.color_palette(colors)\n",
    "    \n",
    "    sns.heatmap(summary_df,\n",
    "                cmap=custom_cmap,\n",
    "                cbar_kws={'label': 'Percentage of \"Yes\" Responses'},\n",
    "                annot=True,\n",
    "                fmt='.0f',\n",
    "                square=False,\n",
    "                center=50)\n",
    "    \n",
    "    plt.title('Summary Heatmap: Percentage of Positive Responses by Category', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.subplots_adjust(left=0.4, right=0.95, top=0.95, bottom=0.1)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(save_path, 'summary_heatmap.png'), \n",
    "                dpi=300, \n",
    "                bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary Heatmap:\")\n",
    "summary_df = plot_summary_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save heatmap\n",
    "print(\"\\nSaving summary heatmap...\")\n",
    "summary_df = plot_and_save_summary_heatmap(df, save_dir)\n",
    "\n",
    "print(f\"\\nAll plots saved in: {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
